import numpy as np
rows, cols = 3, 4
rewards = np.full((rows, cols), -0.04)
rewards[1, 3], rewards[2, 3] = -1, 1
values = np.zeros((rows, cols))
gamma, iterations = 0.9, 1000
actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
for _ in range(iterations):
    new_values = np.copy(values)
    for i in range(rows):
        for j in range(cols):
            if (i, j) in [(1, 3), (2, 3)]: continue
            new_values[i, j] = max(rewards[i, j] + gamma * values[max(0, min(i+di, rows-1)), max(0, min(j+dj, cols-1))] for di, dj in actions)
    if np.allclose(values, new_values): break
    values = new_values
policy = np.array([['URDL'[np.argmax([values[max(0, min(i+di, rows-1)), max(0, min(j+dj, cols-1))] for di, dj in actions])] for j in range(cols)] for i in range(rows)])
policy[1, 3], policy[2, 3] = 'F', 'G'  # Fire and Goal
print("Value Matrix:\n", values, "\n\nOptimal Policy:\n", policy)
